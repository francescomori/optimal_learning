# Optimal Learning Protocols: Code and Simulations

This repository contains the code used in the paper:

> **A statistical physics framework for optimal learning**  
> Francesca Mignacco, Francesco Mori  
> [arXiv:2507.07907](https://arxiv.org/abs/2507.07907)

We provide code and simulations for analyzing optimal learning protocols using a theoretical framework that combines tools from statistical physics and optimal control theory. The results include optimal curricula, dropout schedules, and noise schedules in denoising autoencoders.

---

## Overview

Learning dynamics are described via a closed set of ODEs for order parameters in two-layer neural networks trained with online stochastic gradient descent (SGD). Training protocols are optimized using control-theoretic tools to minimize generalization error.

The repository includes simulations for three representative scenarios:

- **Curriculum learning**: optimal task ordering in studentâ€“teacher perceptrons.
- **Dropout regularization**: optimal schedules for node activation in two-layer soft-committee machines.
- **Denoising autoencoders (DAEs)**: optimal noise and data augmentation schedules in shallow DAEs, including applications to MNIST.

---

## Repository Structure

```
â”œâ”€â”€ curriculum/
â”‚   â”œâ”€â”€ curriculum.ipynb               # Optimal schedule (indirect method: Pontryagin)
â”‚   â””â”€â”€ simulations_curriculum.ipynb   # Validation of ODEs via numerical experiments
â”‚
â”œâ”€â”€ dropout/
â”‚   â”œâ”€â”€ dropout.ipynb                  # Optimal schedule (direct method via CasADi)
â”‚   â””â”€â”€ simulations_dropout.ipynb      # Validation of ODEs via numerical experiments
â”‚
â”œâ”€â”€ DAE/
â”‚   â”œâ”€â”€ DAE.ipynb                      # Optimal schedule (direct method via CasADi)
â”‚   â”œâ”€â”€ simulations_DAE.ipynb          # Validation of ODEs via numerical experiments
â”‚   â””â”€â”€ MNIST/
â”‚       â”œâ”€â”€ MNIST_simulations_DAE.ipynb
â”‚       â”œâ”€â”€ MNIST_b_trained.ipynb
â”‚       â””â”€â”€ data/
â”‚           â”œâ”€â”€ schedule_Delta=0.1_Treal=1_eta=5.pickle
â”‚           â”œâ”€â”€ schedule_Delta=0.2_Treal=1_eta=5.pickle
â”‚           â”œâ”€â”€ schedule_Delta=0.3_Treal=1_eta=5.pickle
â”‚           â””â”€â”€ schedule_Delta=0.4_Treal=1_eta=5.pickle
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt  
```

---


Each notebook is self-contained and can be executed top to bottom.

---

## Notebooks Description

- `simulations_*.ipynb`: used to **validate the theoretical ODEs** by comparing them to direct numerical experiments.
- `curriculum.ipynb`: derives optimal training schedules using **indirect optimal control** (Pontryagin's Maximum Principle).
- `dropout.ipynb` and `DAE.ipynb`: derive optimal training schedules using **direct methods via CasADi**.
- `MNIST_simulations_DAE.ipynb` and `MNIST_b_trained.ipynb`: derive and evaluate **optimal denoising schedules for MNIST**.

---


- **Curriculum Learning** â†’ `simulations_curriculum.ipynb`, `curriculum.ipynb`  
- **Dropout Regularization** â†’ `simulations_dropout.ipynb`, `dropout.ipynb`  
- **DAE (Toy model)** â†’ `simulations_DAE.ipynb`, `DAE.ipynb`  
- **DAE (MNIST)** â†’ `MNIST_simulations_DAE.ipynb`, `MNIST_b_trained.ipynb`  
- **Optimal Schedules** â†’ `data/schedules/*.pickle` (used in DAE simulations for MNIST)

---

## ðŸ“„ License

This repository is released under the MIT License.
